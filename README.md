# Natural_Language_Processing_Specialization
This repository contains the materials for natural language processing courses from deep learning ai NLP series.

## Course 1: Natural Language Processing with Classification and Vector Spaces
Topics: Use logistic regression, na√Øve Bayes, and word vectors to implement sentiment analysis, complete analogies & translate words.

### Week1: Logistic Regression
In this class, I learnt to extract features from text into numerical vectors, then build a binary classifer for tweets using a logistic regression.  
Topics: Sentiment analysis, Logistic regression, Data pre-processing, Calculating word frequencies, Feature extraction, Vocabulary creation, Supervised learning
#### Labs:
* [**Language preprocessing**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Classification_and_Vectr_Spaces/Week1_SentimentAnalysiswithLogisticRegression/NLP_C1_W1_lecture_nb_01.html)  

* [**Visualizing word frequencies**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Classification_and_Vectr_Spaces/Week1_SentimentAnalysiswithLogisticRegression/NLP_C1_W1_lecture_nb_02.html)

* [**Visualizing tweets and logistic regession models**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Classification_and_Vectr_Spaces/Week1_SentimentAnalysiswithLogisticRegression/NLP_C1_W1_lecture_nb_03.html)  

#### Assignment:
* [**Sentiment Analysis with Logistic Regression**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Classification_and_Vectr_Spaces/Week1_SentimentAnalysiswithLogisticRegression/C1_W1_Assignment.html)  

### Week2: Naive Bayes
Learn the theory behind Bayes' rule for conditional probabilities, then apply it toward building a Naive Bayes tweet classifier.  
Topics: Error analysis, Naive Bayes inference, Log likelihood, Laplacian smoothing, conditional probabilities, Bayes rule, Sentiment analysis
#### Labs:
* [**Visualizing likelihoods and confidence ellipses**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Classification_and_Vectr_Spaces/Week2_SentimentAnalysiswithNaiveBayes/NLP_C1_W2_lecture_nb_01.html)  

#### Assignment:
* [**Sentiment analysis with naive bayes**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Classification_and_Vectr_Spaces/Week2_SentimentAnalysiswithNaiveBayes/C1_W2_Assignment.html)  

### Week3: Vector Space Models
Vector space models capture semantic meaning and relationships between words. I learnt how to create word vectors that capture dependencies between words, then visualize their relationships in two dimensions using PCA.  
Topics: Covariance matrices, Dimensionality reduction, Principal component analysis, Cosine similarity, Euclidean distance, Co-occurrence matrices, Vector representations, Vector space models
* [**Linear algebra in python with numpy**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Classification_and_Vectr_Spaces/Week3_VectorSpaceModels/NLP_C1_W3_lecture_nb_01.html)  

* [**Manipulating word embeddings**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Classification_and_Vectr_Spaces/Week3_VectorSpaceModels/NLP_C1_W3_lecture_nb_02.html)  

* [**Another explanation about PCA**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Classification_and_Vectr_Spaces/Week3_VectorSpaceModels/NLP_C1_W3_lecture_nb_03.html)  

#### Assignment:
* [**Word Embeddings**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Classification_and_Vectr_Spaces/Week3_VectorSpaceModels/C1_W3_Assignment.html)  

### Week4: Machine Translation
Learnt to transform word vectors and assign them to subsets using locality sensitive hashing, in order to perform machine translation and document search.  
Topics: Gradient descent, Approximate nearest neighbors, Locality sensitive hashing, Hash functions, Hash tables, K nearest neighbors, Document search, Machine translation, Frobenius norm
#### Labs:
* [**Rotation matrices in R2**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Classification_and_Vectr_Spaces/Week4_MachineTranslationandDocumentSearch/NLP_C1_W4_lecture_nb_01.html)  

* [**Hash tables**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Classification_and_Vectr_Spaces/Week4_MachineTranslationandDocumentSearch/NLP_C1_W4_lecture_nb_02.html)  

#### Assignment:
* [**Word Translation**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Classification_and_Vectr_Spaces/Week4_MachineTranslationandDocumentSearch/C1_W4_Assignment.html)



## Course 2: Natural Language Processing with Probabilistic Models
Topics: Word2vec, Parts-of-Speech tagging, N-gram language models, autocorrect

### Week1: Autocorrect and minimum edit distance.
In this class, I learnt to apply different edit operations (delete, insert, switch, replace) to build a simple auto correct model.
Topics: Autocorrect, minimum edit distance, edit operations.
#### Labs:
* [**Building the Model-Lecture Exercise 01**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Probabilistic_Models/Week1_Autocorrect/NLP_C2_W1_lecture_nb_01.html)  

* [**Building the Model-Lecture Exercise 02**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Probabilistic_Models/Week1_Autocorrect/NLP_C2_W1_lecture_nb_02.html)

#### Assignment:
* [**Autocorrect**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Probabilistic_Models/Week1_Autocorrect/C2_W1_Assignment.html)

### Week2: Part of speech tagging
In this class, the objective is to use Markov chains and hidden markov models to create part-of-speech tags for text corpus.
Topics: Markov chains, Hidden Markov models, Part-of-speech tagging, Viterbi algorithm, Transition probabilities, Emission probabilities.
#### Labs:
* [**Parts-of-Speech Tagging - Working with tags and numpy**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Probabilistic_Models/Week2_PartofSpeechTagging/NLP_C2_W2_lecture_notebook_numpy.html)  

* [**Parts-of-Speech Tagging - First Steps: Working with text files, Creating a vocabulary and Handling unknown words**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Probabilistic_Models/Week2_PartofSpeechTagging/NLP_C2_W2_lecture_notebook_strings_tags.html)

#### Assignment:
* [**Parts-of-Speech Tagging (POS)**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Probabilistic_Models/Week2_PartofSpeechTagging/C2_W2_Assignment.html)

### Week3: Autocomplete and Language Models
Learnt about using N-gram language models by calculating sequence probabilities.
Topics: Language modeling, perplexity, K-smoothing, N-grams, Backoff, Tokenization.
#### Labs:
* [**N-grams Corpus preprocessing**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Probabilistic_Models/Week3_AutocompleteAndLanguageModels/NLP_C2_W3_lecture_nb_01.html)  

* [**Building the language model**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Probabilistic_Models/Week3_AutocompleteAndLanguageModels/NLP_C2_W3_lecture_nb_02.html)

* [**Out of vocabulary words (OOV)**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Probabilistic_Models/Week3_AutocompleteAndLanguageModels/NLP_C2_W3_lecture_nb_03.html)

#### Assignment:
* [**Language Models: Auto-Complete**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Probabilistic_Models/Week3_AutocompleteAndLanguageModels/C2_W3_Assignment.html)

### Week4: Word embeddings with neural networks
Learnt about how to use word embeddings to carry the semantic meaning of words and build continuous-bag-of-words model.    
Topics: Gradient descent, one-hot vectors, neural networks, word embeddings, continuous bag-of-words model, text pre-processing, tokenization.
#### Labs:
* [**Word Embeddings**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Probabilistic_Models/Week4_WordEmbeddingwithNeuralNetworks/NLP_C2_W4_lecture_nb_01.html)  

* [**Word Embeddings First Steps: Data Preparation**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Probabilistic_Models/Week4_WordEmbeddingwithNeuralNetworks/NLP_C2_W4_lecture_notebook_data_prep.html)

* [**Word Embeddings: Intro to CBOW model, activation functions and working with Numpy**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Probabilistic_Models/Week4_WordEmbeddingwithNeuralNetworks/NLP_C2_W4_lecture_notebook_model_architecture.html)  

* [**Word Embeddings: Training the CBOW model**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Probabilistic_Models/Week4_WordEmbeddingwithNeuralNetworks/NLP_C2_W4_lecture_notebook_model_training.html)  

* [**Word Embeddings: Hands On**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Probabilistic_Models/Week4_WordEmbeddingwithNeuralNetworks/NLP_C2_W4_lecture_notebook_word_embeddings.html)

#### Assignment:
* [**Word Embeddings**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_Probabilistic_Models/Week4_WordEmbeddingwithNeuralNetworks/C2_W4_Assignment.html)



## Course 3: Natural Language Processing with Sequence Models
Topics: RNN, GRU, LSTM, NER

### Week1: Neural Networks for Sentiment Analysis.
Learnt about neural networks for deep learning, then build a sophisticated tweet classifier that places tweets into positive or negative sentiment categories, using a deep neural network.
Topics: Feature extraction, Supervised machine learning, Text preprocessing, ReLU, Neural networks.
#### Labs:
* [**Introduction to Trax**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_SequenceModels/Week1_NeuralNetworksforSentimentAnalysis/NLP_C3_W1_lecture_nb_01_trax_intro.html)  

* [**Classes and Subclasses**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_SequenceModels/Week1_NeuralNetworksforSentimentAnalysis/NLP_C3_W1_lecture_nb_02_classes.html)

* [**Data Generators**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_SequenceModels/Week1_NeuralNetworksforSentimentAnalysis/NLP_C3_W1_lecture_nb_03_data_generatos.html)

#### Assignment:
* [**Sentiment with Deep Neural Networks**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_SequenceModels/Week1_NeuralNetworksforSentimentAnalysis/C3_W1_Assignment.html)  


### Week2: Recurrent Neural Networks for Language Modeling.
Learnt about the limitation of traditional language models and see how RNNs and GRUs use sequential data for text prediction.  
Topics: N-grams, Gated recurrent units, Recurrent neural networks.
#### Labs:
* [**Hidden State Activation**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_SequenceModels/Week2_N-gramsVSSequenceModels/C3_W2_Lecture_Notebook_Hidden_State_Activation.html)  

* [**Working with JAX Numpy and Calculating Perplexity**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_SequenceModels/Week2_N-gramsVSSequenceModels/C3_W2_lecture_notebook_perplexity.html)  

* [**Vanilla RNNs, GRUs and the Scan Functions**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_SequenceModels/Week2_N-gramsVSSequenceModels/C3_W2_lecture_notebook_RNNs.html)  

* [**Creating a GRU model using Trax**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_SequenceModels/Week2_N-gramsVSSequenceModels/C3_W2_lecture_notebook_GRU.html)

#### Assignment:
* [**Deep N-grams**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_SequenceModels/Week2_N-gramsVSSequenceModels/C3_W2_Assignment.html)

### Week3: LSTMs and Named Entity Recognition.
Learnt about how long short-term memory units (LSTMs) solve the vanishing gradient problem, and how Named Entity Recognition systems quickly extract important information from text.    
Topics: Vanishing gradients, Named entity recognition, LSTMs, Feature extraction, Part-of-speech tagging, Data generators.
#### Labs:
* [**Vanishing Gradients**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_SequenceModels/Week3_LSTMsandNamedEntityRecognition/C3_W3_Lecture_Notebook_Vanishing_Gradients.html)  

#### Assignment:
* [**Named Entity Recognition**](http://htmlpreview.github.io/?https://github.com/cl3080/Natural_Language_Processing_Specialization/blob/main/NLP_with_SequenceModels/Week3_LSTMsandNamedEntityRecognition/C3_W3_Assignment.html)
